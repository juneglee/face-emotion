{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a79504",
   "metadata": {},
   "source": [
    "#  train emotion (fer2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129ad12",
   "metadata": {},
   "source": [
    "###  load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4642f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33927e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/fer2013/fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21df71ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50bd3b",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecaab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from model.cnn import mini_XCEPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a957d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import DataManager\n",
    "from utils.dataset import split_data\n",
    "from utils.preprocessor import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c066737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "input_shape = (64, 64, 1)\n",
    "validation_split = .2\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "base_path = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2eb318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2656910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 62, 62, 8)    72          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 62, 62, 8)    32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 62, 62, 8)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 60, 8)    576         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 60, 60, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 60, 60, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 60, 60, 16)   200         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 60, 60, 16)   64          separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 60, 60, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 60, 60, 16)   400         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 60, 60, 16)   64          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 30, 30, 16)   128         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 30, 30, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 30, 30, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 30, 30, 16)   0           max_pooling2d[0][0]              \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 30, 30, 32)   656         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 30, 30, 32)   128         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 30, 30, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 30, 30, 32)   1312        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 30, 30, 32)   128         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 15, 15, 32)   512         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15, 15, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15, 15, 32)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 15, 15, 64)   2336        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 15, 64)   256         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 15, 15, 64)   4672        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 15, 15, 64)   256         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 64)     2048        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 64)     256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 64)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 8, 8, 128)    8768        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 8, 8, 128)    17536       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 128)    8192        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 128)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 128)    512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 4, 128)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 4, 7)      8071        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 7)            0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 7)            0           global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 58,423\n",
      "Trainable params: 56,951\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model parameters/compilation\n",
    "model = mini_XCEPTION(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['acc']) # version error ('accuary' >> 'acc')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe2fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: fer2013\n",
      "Epoch 1/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.8148 - acc: 0.3024\n",
      "Epoch 00001: val_loss improved from inf to 1.63840, saving model to ./datasets/fer2013_mini_XCEPTION.01-0.39.hdf5\n",
      "898/897 [==============================] - 538s 599ms/step - loss: 1.8147 - acc: 0.3024 - val_loss: 1.6384 - val_acc: 0.3948\n",
      "Epoch 2/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.5310 - acc: 0.4245\n",
      "Epoch 00002: val_loss improved from 1.63840 to 1.45486, saving model to ./datasets/fer2013_mini_XCEPTION.02-0.43.hdf5\n",
      "898/897 [==============================] - 502s 559ms/step - loss: 1.5312 - acc: 0.4245 - val_loss: 1.4549 - val_acc: 0.4310\n",
      "Epoch 3/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.4023 - acc: 0.4723\n",
      "Epoch 00003: val_loss did not improve from 1.45486\n",
      "898/897 [==============================] - 426s 474ms/step - loss: 1.4020 - acc: 0.4725 - val_loss: 1.4979 - val_acc: 0.4550\n",
      "Epoch 4/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.3281 - acc: 0.5018\n",
      "Epoch 00004: val_loss did not improve from 1.45486\n",
      "898/897 [==============================] - 406s 453ms/step - loss: 1.3282 - acc: 0.5018 - val_loss: 1.5985 - val_acc: 0.4714\n",
      "Epoch 5/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.2724 - acc: 0.5211\n",
      "Epoch 00005: val_loss improved from 1.45486 to 1.34576, saving model to ./datasets/fer2013_mini_XCEPTION.05-0.51.hdf5\n",
      "898/897 [==============================] - 400s 445ms/step - loss: 1.2724 - acc: 0.5211 - val_loss: 1.3458 - val_acc: 0.5067\n",
      "Epoch 6/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.2315 - acc: 0.5374\n",
      "Epoch 00006: val_loss improved from 1.34576 to 1.24224, saving model to ./datasets/fer2013_mini_XCEPTION.06-0.53.hdf5\n",
      "898/897 [==============================] - 402s 448ms/step - loss: 1.2315 - acc: 0.5375 - val_loss: 1.2422 - val_acc: 0.5325\n",
      "Epoch 7/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.2004 - acc: 0.5489\n",
      "Epoch 00007: val_loss did not improve from 1.24224\n",
      "898/897 [==============================] - 400s 446ms/step - loss: 1.2004 - acc: 0.5489 - val_loss: 1.2645 - val_acc: 0.5373\n",
      "Epoch 8/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1842 - acc: 0.5564\n",
      "Epoch 00008: val_loss did not improve from 1.24224\n",
      "898/897 [==============================] - 387s 431ms/step - loss: 1.1844 - acc: 0.5564 - val_loss: 1.3023 - val_acc: 0.5231\n",
      "Epoch 9/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1571 - acc: 0.5662\n",
      "Epoch 00009: val_loss improved from 1.24224 to 1.16652, saving model to ./datasets/fer2013_mini_XCEPTION.09-0.57.hdf5\n",
      "898/897 [==============================] - 375s 418ms/step - loss: 1.1573 - acc: 0.5661 - val_loss: 1.1665 - val_acc: 0.5683\n",
      "Epoch 10/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1357 - acc: 0.5758\n",
      "Epoch 00010: val_loss did not improve from 1.16652\n",
      "898/897 [==============================] - 375s 417ms/step - loss: 1.1358 - acc: 0.5757 - val_loss: 1.2022 - val_acc: 0.5600\n",
      "Epoch 11/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1255 - acc: 0.5770\n",
      "Epoch 00011: val_loss did not improve from 1.16652\n",
      "898/897 [==============================] - 374s 416ms/step - loss: 1.1253 - acc: 0.5771 - val_loss: 1.3116 - val_acc: 0.5202\n",
      "Epoch 12/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1105 - acc: 0.5851\n",
      "Epoch 00012: val_loss did not improve from 1.16652\n",
      "898/897 [==============================] - 373s 415ms/step - loss: 1.1105 - acc: 0.5851 - val_loss: 1.2098 - val_acc: 0.5683\n",
      "Epoch 13/100\n",
      "897/897 [============================>.] - ETA: 7s - loss: 1.0983 - acc: 0.5902 \n",
      "Epoch 00013: val_loss improved from 1.16652 to 1.16571, saving model to ./datasets/fer2013_mini_XCEPTION.13-0.57.hdf5\n",
      "898/897 [==============================] - 45002s 50s/step - loss: 1.0984 - acc: 0.5901 - val_loss: 1.1657 - val_acc: 0.5715\n",
      "Epoch 14/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0905 - acc: 0.5953\n",
      "Epoch 00014: val_loss improved from 1.16571 to 1.13093, saving model to ./datasets/fer2013_mini_XCEPTION.14-0.59.hdf5\n",
      "898/897 [==============================] - 379s 422ms/step - loss: 1.0904 - acc: 0.5952 - val_loss: 1.1309 - val_acc: 0.5860\n",
      "Epoch 15/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0760 - acc: 0.5992\n",
      "Epoch 00015: val_loss improved from 1.13093 to 1.08972, saving model to ./datasets/fer2013_mini_XCEPTION.15-0.59.hdf5\n",
      "898/897 [==============================] - 382s 426ms/step - loss: 1.0758 - acc: 0.5993 - val_loss: 1.0897 - val_acc: 0.5906\n",
      "Epoch 16/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0632 - acc: 0.6040\n",
      "Epoch 00016: val_loss did not improve from 1.08972\n",
      "898/897 [==============================] - 386s 430ms/step - loss: 1.0632 - acc: 0.6040 - val_loss: 1.1047 - val_acc: 0.5968\n",
      "Epoch 17/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0512 - acc: 0.6069\n",
      "Epoch 00017: val_loss did not improve from 1.08972\n",
      "898/897 [==============================] - 393s 437ms/step - loss: 1.0513 - acc: 0.6070 - val_loss: 1.1342 - val_acc: 0.5828\n",
      "Epoch 18/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0501 - acc: 0.6069\n",
      "Epoch 00018: val_loss did not improve from 1.08972\n",
      "898/897 [==============================] - 393s 438ms/step - loss: 1.0498 - acc: 0.6070 - val_loss: 1.1573 - val_acc: 0.5784\n",
      "Epoch 19/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0421 - acc: 0.6145\n",
      "Epoch 00019: val_loss did not improve from 1.08972\n",
      "898/897 [==============================] - 394s 439ms/step - loss: 1.0419 - acc: 0.6145 - val_loss: 1.1247 - val_acc: 0.5865\n",
      "Epoch 20/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0337 - acc: 0.6157\n",
      "Epoch 00020: val_loss did not improve from 1.08972\n",
      "898/897 [==============================] - 372s 415ms/step - loss: 1.0336 - acc: 0.6158 - val_loss: 1.1409 - val_acc: 0.5853\n",
      "Epoch 21/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0235 - acc: 0.6181\n",
      "Epoch 00021: val_loss did not improve from 1.08972\n",
      "898/897 [==============================] - 381s 424ms/step - loss: 1.0234 - acc: 0.6181 - val_loss: 1.1396 - val_acc: 0.5910\n",
      "Epoch 22/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0226 - acc: 0.6187\n",
      "Epoch 00022: val_loss did not improve from 1.08972\n",
      "898/897 [==============================] - 382s 425ms/step - loss: 1.0224 - acc: 0.6188 - val_loss: 1.0900 - val_acc: 0.6059\n",
      "Epoch 23/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0096 - acc: 0.6229\n",
      "Epoch 00023: val_loss did not improve from 1.08972\n",
      "898/897 [==============================] - 374s 416ms/step - loss: 1.0098 - acc: 0.6229 - val_loss: 1.1644 - val_acc: 0.5776\n",
      "Epoch 24/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0126 - acc: 0.6218\n",
      "Epoch 00024: val_loss improved from 1.08972 to 1.07873, saving model to ./datasets/fer2013_mini_XCEPTION.24-0.60.hdf5\n",
      "898/897 [==============================] - 751s 837ms/step - loss: 1.0124 - acc: 0.6218 - val_loss: 1.0787 - val_acc: 0.5970\n",
      "Epoch 25/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0023 - acc: 0.6282\n",
      "Epoch 00025: val_loss improved from 1.07873 to 1.05931, saving model to ./datasets/fer2013_mini_XCEPTION.25-0.62.hdf5\n",
      "898/897 [==============================] - 435s 485ms/step - loss: 1.0020 - acc: 0.6283 - val_loss: 1.0593 - val_acc: 0.6169\n",
      "Epoch 26/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9967 - acc: 0.6309\n",
      "Epoch 00026: val_loss did not improve from 1.05931\n",
      "898/897 [==============================] - 481s 536ms/step - loss: 0.9969 - acc: 0.6307 - val_loss: 1.1173 - val_acc: 0.5956\n",
      "Epoch 27/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9922 - acc: 0.6282\n",
      "Epoch 00027: val_loss did not improve from 1.05931\n",
      "898/897 [==============================] - 448s 499ms/step - loss: 0.9924 - acc: 0.6281 - val_loss: 1.0696 - val_acc: 0.6066\n",
      "Epoch 28/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9869 - acc: 0.6326\n",
      "Epoch 00028: val_loss did not improve from 1.05931\n",
      "898/897 [==============================] - 450s 501ms/step - loss: 0.9870 - acc: 0.6326 - val_loss: 1.0838 - val_acc: 0.6113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9829 - acc: 0.6363\n",
      "Epoch 00029: val_loss did not improve from 1.05931\n",
      "898/897 [==============================] - 515s 574ms/step - loss: 0.9828 - acc: 0.6363 - val_loss: 1.0965 - val_acc: 0.6124\n",
      "Epoch 30/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9782 - acc: 0.6362\n",
      "Epoch 00030: val_loss did not improve from 1.05931\n",
      "898/897 [==============================] - 478s 533ms/step - loss: 0.9783 - acc: 0.6362 - val_loss: 1.0932 - val_acc: 0.6016\n",
      "Epoch 31/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9746 - acc: 0.6377\n",
      "Epoch 00031: val_loss improved from 1.05931 to 1.05470, saving model to ./datasets/fer2013_mini_XCEPTION.31-0.62.hdf5\n",
      "898/897 [==============================] - 512s 570ms/step - loss: 0.9747 - acc: 0.6376 - val_loss: 1.0547 - val_acc: 0.6154\n",
      "Epoch 32/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9719 - acc: 0.6388\n",
      "Epoch 00032: val_loss improved from 1.05470 to 1.04817, saving model to ./datasets/fer2013_mini_XCEPTION.32-0.62.hdf5\n",
      "898/897 [==============================] - 480s 534ms/step - loss: 0.9719 - acc: 0.6389 - val_loss: 1.0482 - val_acc: 0.6184\n",
      "Epoch 33/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9718 - acc: 0.6385\n",
      "Epoch 00033: val_loss did not improve from 1.04817\n",
      "898/897 [==============================] - 448s 499ms/step - loss: 0.9718 - acc: 0.6385 - val_loss: 1.0916 - val_acc: 0.6127\n",
      "Epoch 34/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9630 - acc: 0.6401\n",
      "Epoch 00034: val_loss improved from 1.04817 to 1.03353, saving model to ./datasets/fer2013_mini_XCEPTION.34-0.62.hdf5\n",
      "898/897 [==============================] - 453s 504ms/step - loss: 0.9627 - acc: 0.6403 - val_loss: 1.0335 - val_acc: 0.6195\n",
      "Epoch 35/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9566 - acc: 0.6448\n",
      "Epoch 00035: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 472s 525ms/step - loss: 0.9565 - acc: 0.6448 - val_loss: 1.0349 - val_acc: 0.6190\n",
      "Epoch 36/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9498 - acc: 0.6447\n",
      "Epoch 00036: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 494s 550ms/step - loss: 0.9499 - acc: 0.6447 - val_loss: 1.0392 - val_acc: 0.6191\n",
      "Epoch 37/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9548 - acc: 0.6443\n",
      "Epoch 00037: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 492s 548ms/step - loss: 0.9547 - acc: 0.6443 - val_loss: 1.0455 - val_acc: 0.6177\n",
      "Epoch 38/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9458 - acc: 0.6490\n",
      "Epoch 00038: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 487s 543ms/step - loss: 0.9458 - acc: 0.6490 - val_loss: 1.0676 - val_acc: 0.6117\n",
      "Epoch 39/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9471 - acc: 0.6486\n",
      "Epoch 00039: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 498s 555ms/step - loss: 0.9471 - acc: 0.6485 - val_loss: 1.0766 - val_acc: 0.6113\n",
      "Epoch 40/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9387 - acc: 0.6502\n",
      "Epoch 00040: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 528s 588ms/step - loss: 0.9389 - acc: 0.6501 - val_loss: 1.0398 - val_acc: 0.6141\n",
      "Epoch 41/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9398 - acc: 0.6519\n",
      "Epoch 00041: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 500s 557ms/step - loss: 0.9393 - acc: 0.6521 - val_loss: 1.0345 - val_acc: 0.6273\n",
      "Epoch 42/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9381 - acc: 0.6536\n",
      "Epoch 00042: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 493s 549ms/step - loss: 0.9381 - acc: 0.6534 - val_loss: 1.0396 - val_acc: 0.6216\n",
      "Epoch 43/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9338 - acc: 0.6529\n",
      "Epoch 00043: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 496s 552ms/step - loss: 0.9338 - acc: 0.6528 - val_loss: 1.0689 - val_acc: 0.6158\n",
      "Epoch 44/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9295 - acc: 0.6514\n",
      "Epoch 00044: val_loss did not improve from 1.03353\n",
      "898/897 [==============================] - 531s 591ms/step - loss: 0.9295 - acc: 0.6514 - val_loss: 1.1094 - val_acc: 0.5963\n",
      "Epoch 45/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9255 - acc: 0.6599\n",
      "Epoch 00045: val_loss improved from 1.03353 to 1.02315, saving model to ./datasets/fer2013_mini_XCEPTION.45-0.64.hdf5\n",
      "898/897 [==============================] - 560s 623ms/step - loss: 0.9250 - acc: 0.6601 - val_loss: 1.0231 - val_acc: 0.6362\n",
      "Epoch 46/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.6572\n",
      "Epoch 00046: val_loss did not improve from 1.02315\n",
      "898/897 [==============================] - 509s 567ms/step - loss: 0.9253 - acc: 0.6571 - val_loss: 1.1218 - val_acc: 0.6081\n",
      "Epoch 47/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9213 - acc: 0.6591\n",
      "Epoch 00047: val_loss did not improve from 1.02315\n",
      "898/897 [==============================] - 465s 517ms/step - loss: 0.9211 - acc: 0.6592 - val_loss: 1.0432 - val_acc: 0.6240\n",
      "Epoch 48/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9260 - acc: 0.6569\n",
      "Epoch 00048: val_loss improved from 1.02315 to 1.01540, saving model to ./datasets/fer2013_mini_XCEPTION.48-0.64.hdf5\n",
      "898/897 [==============================] - 471s 524ms/step - loss: 0.9261 - acc: 0.6568 - val_loss: 1.0154 - val_acc: 0.6353\n",
      "Epoch 49/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9214 - acc: 0.6608\n",
      "Epoch 00049: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 467s 520ms/step - loss: 0.9214 - acc: 0.6607 - val_loss: 1.0257 - val_acc: 0.6257\n",
      "Epoch 50/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9168 - acc: 0.6587\n",
      "Epoch 00050: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 465s 518ms/step - loss: 0.9169 - acc: 0.6587 - val_loss: 1.0329 - val_acc: 0.6223\n",
      "Epoch 51/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9187 - acc: 0.6590\n",
      "Epoch 00051: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 468s 521ms/step - loss: 0.9187 - acc: 0.6590 - val_loss: 1.0295 - val_acc: 0.6195\n",
      "Epoch 52/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9089 - acc: 0.6614\n",
      "Epoch 00052: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 465s 518ms/step - loss: 0.9089 - acc: 0.6613 - val_loss: 1.0159 - val_acc: 0.6365\n",
      "Epoch 53/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9109 - acc: 0.6601\n",
      "Epoch 00053: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 468s 521ms/step - loss: 0.9108 - acc: 0.6602 - val_loss: 1.0710 - val_acc: 0.6170\n",
      "Epoch 54/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9037 - acc: 0.6654\n",
      "Epoch 00054: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 463s 515ms/step - loss: 0.9040 - acc: 0.6652 - val_loss: 1.1237 - val_acc: 0.6006\n",
      "Epoch 55/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9009 - acc: 0.6645\n",
      "Epoch 00055: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 466s 519ms/step - loss: 0.9009 - acc: 0.6645 - val_loss: 1.0636 - val_acc: 0.6102\n",
      "Epoch 56/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.9063 - acc: 0.6646\n",
      "Epoch 00056: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 464s 517ms/step - loss: 0.9067 - acc: 0.6644 - val_loss: 1.0358 - val_acc: 0.6275\n",
      "Epoch 57/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8996 - acc: 0.6689\n",
      "Epoch 00057: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 463s 515ms/step - loss: 0.8996 - acc: 0.6690 - val_loss: 1.0269 - val_acc: 0.6290\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [============================>.] - ETA: 0s - loss: 0.8976 - acc: 0.6700\n",
      "Epoch 00058: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 461s 513ms/step - loss: 0.8978 - acc: 0.6699 - val_loss: 1.1069 - val_acc: 0.6069\n",
      "Epoch 59/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8953 - acc: 0.6680\n",
      "Epoch 00059: val_loss did not improve from 1.01540\n",
      "898/897 [==============================] - 458s 510ms/step - loss: 0.8950 - acc: 0.6682 - val_loss: 1.0241 - val_acc: 0.6269\n",
      "Epoch 60/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8957 - acc: 0.6700\n",
      "Epoch 00060: val_loss did not improve from 1.01540\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "898/897 [==============================] - 458s 510ms/step - loss: 0.8961 - acc: 0.6698 - val_loss: 1.0388 - val_acc: 0.6305\n",
      "Epoch 61/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8448 - acc: 0.6871\n",
      "Epoch 00061: val_loss improved from 1.01540 to 0.98063, saving model to ./datasets/fer2013_mini_XCEPTION.61-0.65.hdf5\n",
      "898/897 [==============================] - 470s 523ms/step - loss: 0.8447 - acc: 0.6872 - val_loss: 0.9806 - val_acc: 0.6471\n",
      "Epoch 62/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8257 - acc: 0.6963\n",
      "Epoch 00062: val_loss improved from 0.98063 to 0.97323, saving model to ./datasets/fer2013_mini_XCEPTION.62-0.65.hdf5\n",
      "898/897 [==============================] - 458s 510ms/step - loss: 0.8257 - acc: 0.6963 - val_loss: 0.9732 - val_acc: 0.6505\n",
      "Epoch 63/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8210 - acc: 0.6982\n",
      "Epoch 00063: val_loss did not improve from 0.97323\n",
      "898/897 [==============================] - 456s 508ms/step - loss: 0.8214 - acc: 0.6981 - val_loss: 0.9789 - val_acc: 0.6481\n",
      "Epoch 64/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8161 - acc: 0.7007\n",
      "Epoch 00064: val_loss improved from 0.97323 to 0.97247, saving model to ./datasets/fer2013_mini_XCEPTION.64-0.65.hdf5\n",
      "898/897 [==============================] - 458s 511ms/step - loss: 0.8161 - acc: 0.7006 - val_loss: 0.9725 - val_acc: 0.6489\n",
      "Epoch 65/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.7002\n",
      "Epoch 00065: val_loss did not improve from 0.97247\n",
      "898/897 [==============================] - 459s 511ms/step - loss: 0.8161 - acc: 0.7001 - val_loss: 0.9751 - val_acc: 0.6495\n",
      "Epoch 66/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.7004\n",
      "Epoch 00066: val_loss improved from 0.97247 to 0.97245, saving model to ./datasets/fer2013_mini_XCEPTION.66-0.65.hdf5\n",
      "898/897 [==============================] - 457s 509ms/step - loss: 0.8145 - acc: 0.7003 - val_loss: 0.9724 - val_acc: 0.6495\n",
      "Epoch 67/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8130 - acc: 0.6996\n",
      "Epoch 00067: val_loss improved from 0.97245 to 0.97106, saving model to ./datasets/fer2013_mini_XCEPTION.67-0.65.hdf5\n",
      "898/897 [==============================] - 462s 514ms/step - loss: 0.8129 - acc: 0.6996 - val_loss: 0.9711 - val_acc: 0.6506\n",
      "Epoch 68/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8100 - acc: 0.7020\n",
      "Epoch 00068: val_loss improved from 0.97106 to 0.97007, saving model to ./datasets/fer2013_mini_XCEPTION.68-0.65.hdf5\n",
      "898/897 [==============================] - 457s 509ms/step - loss: 0.8101 - acc: 0.7020 - val_loss: 0.9701 - val_acc: 0.6464\n",
      "Epoch 69/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8045 - acc: 0.7021\n",
      "Epoch 00069: val_loss did not improve from 0.97007\n",
      "898/897 [==============================] - 419s 466ms/step - loss: 0.8046 - acc: 0.7022 - val_loss: 0.9772 - val_acc: 0.6493\n",
      "Epoch 70/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8062 - acc: 0.7002\n",
      "Epoch 00070: val_loss did not improve from 0.97007\n",
      "898/897 [==============================] - 418s 465ms/step - loss: 0.8063 - acc: 0.7001 - val_loss: 0.9768 - val_acc: 0.6506\n",
      "Epoch 71/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8004 - acc: 0.7019\n",
      "Epoch 00071: val_loss improved from 0.97007 to 0.96986, saving model to ./datasets/fer2013_mini_XCEPTION.71-0.65.hdf5\n",
      "898/897 [==============================] - 425s 473ms/step - loss: 0.8005 - acc: 0.7019 - val_loss: 0.9699 - val_acc: 0.6516\n",
      "Epoch 72/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8041 - acc: 0.7010\n",
      "Epoch 00072: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 449s 499ms/step - loss: 0.8040 - acc: 0.7011 - val_loss: 0.9752 - val_acc: 0.6485\n",
      "Epoch 73/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7969 - acc: 0.7063\n",
      "Epoch 00073: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 447s 498ms/step - loss: 0.7966 - acc: 0.7064 - val_loss: 0.9768 - val_acc: 0.6484\n",
      "Epoch 74/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.8000 - acc: 0.7027\n",
      "Epoch 00074: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 448s 499ms/step - loss: 0.8003 - acc: 0.7026 - val_loss: 0.9771 - val_acc: 0.6492\n",
      "Epoch 75/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7960 - acc: 0.7046\n",
      "Epoch 00075: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 458s 510ms/step - loss: 0.7961 - acc: 0.7046 - val_loss: 0.9801 - val_acc: 0.6485\n",
      "Epoch 76/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7984 - acc: 0.7071\n",
      "Epoch 00076: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 449s 500ms/step - loss: 0.7987 - acc: 0.7071 - val_loss: 0.9754 - val_acc: 0.6520\n",
      "Epoch 77/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7931 - acc: 0.7071\n",
      "Epoch 00077: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 447s 498ms/step - loss: 0.7931 - acc: 0.7072 - val_loss: 0.9742 - val_acc: 0.6480\n",
      "Epoch 78/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7931 - acc: 0.7062\n",
      "Epoch 00078: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 451s 502ms/step - loss: 0.7930 - acc: 0.7063 - val_loss: 0.9762 - val_acc: 0.6493\n",
      "Epoch 79/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7890 - acc: 0.7111\n",
      "Epoch 00079: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 448s 499ms/step - loss: 0.7888 - acc: 0.7112 - val_loss: 0.9803 - val_acc: 0.6480\n",
      "Epoch 80/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7964 - acc: 0.7011\n",
      "Epoch 00080: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 412s 459ms/step - loss: 0.7964 - acc: 0.7011 - val_loss: 0.9752 - val_acc: 0.6485\n",
      "Epoch 81/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7909 - acc: 0.7085\n",
      "Epoch 00081: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 394s 438ms/step - loss: 0.7909 - acc: 0.7085 - val_loss: 0.9780 - val_acc: 0.6503\n",
      "Epoch 82/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7924 - acc: 0.7080\n",
      "Epoch 00082: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 393s 437ms/step - loss: 0.7922 - acc: 0.7081 - val_loss: 0.9768 - val_acc: 0.6509\n",
      "Epoch 83/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7904 - acc: 0.7062\n",
      "Epoch 00083: val_loss did not improve from 0.96986\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "898/897 [==============================] - 394s 438ms/step - loss: 0.7905 - acc: 0.7062 - val_loss: 0.9797 - val_acc: 0.6470\n",
      "Epoch 84/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7870 - acc: 0.7086\n",
      "Epoch 00084: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 410s 456ms/step - loss: 0.7872 - acc: 0.7085 - val_loss: 0.9749 - val_acc: 0.6496\n",
      "Epoch 85/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7840 - acc: 0.7113\n",
      "Epoch 00085: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 445s 496ms/step - loss: 0.7840 - acc: 0.7113 - val_loss: 0.9729 - val_acc: 0.6516\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [============================>.] - ETA: 0s - loss: 0.7817 - acc: 0.7122\n",
      "Epoch 00086: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 421s 469ms/step - loss: 0.7814 - acc: 0.7123 - val_loss: 0.9728 - val_acc: 0.6506\n",
      "Epoch 87/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7852 - acc: 0.7111\n",
      "Epoch 00087: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 405s 452ms/step - loss: 0.7850 - acc: 0.7112 - val_loss: 0.9723 - val_acc: 0.6510\n",
      "Epoch 88/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7851 - acc: 0.7085\n",
      "Epoch 00088: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 1157s 1s/step - loss: 0.7851 - acc: 0.7085 - val_loss: 0.9735 - val_acc: 0.6498\n",
      "Epoch 89/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7832 - acc: 0.7141\n",
      "Epoch 00089: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 440s 489ms/step - loss: 0.7833 - acc: 0.7141 - val_loss: 0.9727 - val_acc: 0.6514\n",
      "Epoch 90/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7875 - acc: 0.7110\n",
      "Epoch 00090: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 429s 478ms/step - loss: 0.7873 - acc: 0.7110 - val_loss: 0.9726 - val_acc: 0.6500\n",
      "Epoch 91/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7813 - acc: 0.7111\n",
      "Epoch 00091: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 440s 490ms/step - loss: 0.7815 - acc: 0.7110 - val_loss: 0.9724 - val_acc: 0.6516\n",
      "Epoch 92/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7818 - acc: 0.7098\n",
      "Epoch 00092: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 435s 485ms/step - loss: 0.7819 - acc: 0.7098 - val_loss: 0.9739 - val_acc: 0.6514\n",
      "Epoch 93/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7828 - acc: 0.7095\n",
      "Epoch 00093: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 451s 502ms/step - loss: 0.7830 - acc: 0.7094 - val_loss: 0.9728 - val_acc: 0.6512\n",
      "Epoch 94/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7832 - acc: 0.7103\n",
      "Epoch 00094: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 408s 454ms/step - loss: 0.7831 - acc: 0.7103 - val_loss: 0.9739 - val_acc: 0.6499\n",
      "Epoch 95/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7738 - acc: 0.7164\n",
      "Epoch 00095: val_loss did not improve from 0.96986\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "898/897 [==============================] - 420s 467ms/step - loss: 0.7736 - acc: 0.7164 - val_loss: 0.9743 - val_acc: 0.6493\n",
      "Epoch 96/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7822 - acc: 0.7104\n",
      "Epoch 00096: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 459s 511ms/step - loss: 0.7823 - acc: 0.7103 - val_loss: 0.9742 - val_acc: 0.6496\n",
      "Epoch 97/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7788 - acc: 0.7129\n",
      "Epoch 00097: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 470s 523ms/step - loss: 0.7790 - acc: 0.7129 - val_loss: 0.9742 - val_acc: 0.6510\n",
      "Epoch 98/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7787 - acc: 0.7117\n",
      "Epoch 00098: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 459s 511ms/step - loss: 0.7792 - acc: 0.7115 - val_loss: 0.9741 - val_acc: 0.6498\n",
      "Epoch 99/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7787 - acc: 0.7135\n",
      "Epoch 00099: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 421s 469ms/step - loss: 0.7785 - acc: 0.7136 - val_loss: 0.9736 - val_acc: 0.6505\n",
      "Epoch 100/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 0.7779 - acc: 0.7121\n",
      "Epoch 00100: val_loss did not improve from 0.96986\n",
      "898/897 [==============================] - 394s 439ms/step - loss: 0.7778 - acc: 0.7123 - val_loss: 0.9740 - val_acc: 0.6502\n"
     ]
    }
   ],
   "source": [
    "datasets = ['fer2013']\n",
    "for dataset_name in datasets:\n",
    "    print('Training dataset:', dataset_name)\n",
    "\n",
    "    # callbacks\n",
    "    log_file_path = base_path + dataset_name + '_emotion_training.log'\n",
    "    csv_logger = CSVLogger(log_file_path, append=False)\n",
    "    early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
    "                                  patience=int(patience/4), verbose=1)\n",
    "    trained_models_path = base_path + dataset_name + '_mini_XCEPTION'\n",
    "    model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
    "                                                    save_best_only=True)\n",
    "    callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "    # loading dataset\n",
    "    data_loader = DataManager(dataset_name, image_size=input_shape[:2])\n",
    "    faces, emotions = data_loader.get_data()\n",
    "    faces = preprocess_input(faces)\n",
    "    num_samples, num_classes = emotions.shape\n",
    "    train_data, val_data = split_data(faces, emotions, validation_split)\n",
    "    train_faces, train_emotions = train_data\n",
    "    model.fit_generator(data_generator.flow(train_faces, train_emotions,\n",
    "                                            batch_size),\n",
    "                        steps_per_epoch=len(train_faces) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37b9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
