{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a79504",
   "metadata": {},
   "source": [
    "#  train emotion (fer2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129ad12",
   "metadata": {},
   "source": [
    "###  load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4642f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33927e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/fer2013/fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21df71ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50bd3b",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecaab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from model.cnn import mini_XCEPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a957d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import DataManager\n",
    "from utils.dataset import split_data\n",
    "from utils.preprocessor import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c066737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "input_shape = (64, 64, 1)\n",
    "validation_split = .2\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "base_path = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0834d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449d588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 62, 62, 8)    72          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 62, 62, 8)    32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 62, 62, 8)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 60, 8)    576         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 60, 60, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 60, 60, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 60, 60, 16)   200         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 60, 60, 16)   64          separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 60, 60, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 60, 60, 16)   400         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 60, 60, 16)   64          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 30, 30, 16)   128         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 30, 30, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 30, 30, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 30, 30, 16)   0           max_pooling2d[0][0]              \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 30, 30, 32)   656         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 30, 30, 32)   128         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 30, 30, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 30, 30, 32)   1312        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 30, 30, 32)   128         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 15, 15, 32)   512         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15, 15, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15, 15, 32)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 15, 15, 64)   2336        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 15, 64)   256         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 15, 15, 64)   4672        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 15, 15, 64)   256         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 64)     2048        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 64)     256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 64)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 8, 8, 128)    8768        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 8, 8, 128)    17536       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 128)    8192        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 128)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 128)    512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 4, 128)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 4, 7)      8071        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 7)            0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 7)            0           global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 58,423\n",
      "Trainable params: 56,951\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model parameters/compilation\n",
    "model = mini_XCEPTION(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['acc']) # version error ('accuary' >> 'acc')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7ce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: fer2013\n",
      "Epoch 1/100\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.7672 - acc: 0.3316\n",
      "Epoch 00001: val_loss improved from inf to 1.67245, saving model to ./datasetsfer2013_mini_XCEPTION.01-0.40.hdf5\n",
      "898/897 [==============================] - 493s 549ms/step - loss: 1.7671 - acc: 0.3316 - val_loss: 1.6724 - val_acc: 0.3951\n",
      "Epoch 2/100\n",
      "470/897 [==============>...............] - ETA: 3:22 - loss: 1.5469 - acc: 0.4222"
     ]
    }
   ],
   "source": [
    "datasets = ['fer2013']\n",
    "for dataset_name in datasets:\n",
    "    print('Training dataset:', dataset_name)\n",
    "\n",
    "    # callbacks\n",
    "    log_file_path = base_path + dataset_name + '_emotion_training.log'\n",
    "    csv_logger = CSVLogger(log_file_path, append=False)\n",
    "    early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
    "                                  patience=int(patience/4), verbose=1)\n",
    "    trained_models_path = base_path + dataset_name + '_mini_XCEPTION'\n",
    "    model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
    "                                                    save_best_only=True)\n",
    "    callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "    # loading dataset\n",
    "    data_loader = DataManager(dataset_name, image_size=input_shape[:2])\n",
    "    faces, emotions = data_loader.get_data()\n",
    "    faces = preprocess_input(faces)\n",
    "    num_samples, num_classes = emotions.shape\n",
    "    train_data, val_data = split_data(faces, emotions, validation_split)\n",
    "    train_faces, train_emotions = train_data\n",
    "    model.fit_generator(data_generator.flow(train_faces, train_emotions,\n",
    "                                            batch_size),\n",
    "                        steps_per_epoch=len(train_faces) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedea48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
